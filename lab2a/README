NAME: Stephanie Doan
EMAIL: stephaniekdoan@ucla.edu
ID: 604981556

INCLUDED FILES
- lab2_add.c: source code for program that adds 1 and -1 to a counter with variable number of threads and iterations, with options for mutex, spin-lock, compare-and-swap, and no synchronization
- lab2_list.c: source code for program that inserts and deletes nodes from a linked list with a variable number of threads and iterations, with options for mutex, spin-lock, and no synchronization
- SortedList.h: header file for sorted linked list, with node insertion, lookup, deletion, and length methods
- SortedList.c: implementations for linked list methods in SortedList.h
- lab2_add.csv: data generated by lab2_add program run with different combinations of arguments
- lab2_add-1.png: threads and iterations that run without failure for lab2_add
- lab2_add-2.png: cost of yielding for lab2_add 
- lab2_add-3.png: per-thread cost vs number of iteraions for lab2_add
- lab2_add-4.png: threads and iterations that run without failure for lab2_add
- lab2_add-5.png: per operation cost vs number of threads for lab2_add
- lab2_list.csv: data generated by lab2_list program run with different combinations of arguments
- lab2_list-1.png: cost per operation vs iterations for lab2_list
- lab2_list-2.png: unprotected threads and iterations that run without failure for lab2_list 
- lab2_list-3.png: protected iterations that run without failure for lab2_list
- lab2_list-4.png: scalability of synchronization and mechanisms for lab2_list
- lab2_add.gp: gnuplot data reduction script for lab2_add results
- lab2_list.gp: gnuplot data reduction script for lab2_list results
- Makefile: contains target to build lab2_add and lab2_list programs, generate data, make gnu plots, clean, and create tarball
- README: this file

QUESTIONS
2.1.1 - causing conflicts
- Why does it take many iterations before errors are seen?
    Errors are created by race conditions between threads that increment and decrement the counter at the same time, resulting a non-zero final value. It takes many iterations to produce errors because a high number of iterations increases the probability that multiple threads modify the counter simultaneously.
- Why does a significantly smaller number of iterations so seldom fail?
    A small number of iterations lowers the probability that multiple increment and decrement operations are executed concurrently, since fewer of these data race inducing operations are carried out through the program.

2.1.2 - cost of yielding
- Why are the --yield runs so much slower?
    As explained in the sched_yield man page, sched_yield() makes the calling thread relinquish the CPU for other threads to run. Overhead from frequent thread context switches make the program slower with the --yield flag.
- Where is the additional time going?
    The time is going to the CPU switching from the execution of one thread to the execution of another, which includes saving data, program pointer, and other structures of the current thread and loading the structures of the other thread to run.
- Is it possible to get valid per-operation timings if we are using the --yield option? If so, explain how. If not, explain why not.
    No, we cannot time each operation separately because we cannot take into account the time for the context switches accurately for each operation.

2.1.3 - measurement errors
- Why does the average cost per operation drop with increasing iterations?
    Higher number of iterations amortizes the (relatively high) cost of creating a new thread, since each thread would execute more iterations while the number of threads created stays the same.
- If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?
    The ideal number of iterations can be found by running a very high number of iterations and seeing where the cost per operation slows its decrease and levels out, such that further increase in iteration count will not result in saving any more cost. This point is the "correct" cost.

2.1.4 - costs of serialization
- Why do all of the options perform similarly for low numbers of threads?
    Fewer threads means less contention to enter the critical section, overall less time for each thread waiting for control to increment or decrement the counter, so we see little difference in performance between the options.
- Why do the three protected operations slow down as the number of threads rises?
    More threads contend for control of the critical section, so each thread waits a long time behind many other threads to gain control of the lock. More time is spent waiting for a lock than executing increments and decrements.

2.2.1 - scalability of Mutex
    Per-operation cost for mutex-protected operations increases more with the number of threads in Part 1 (add) than in Part 2 (sorted lists). As shown in lab2_add-5.png, the blue mutex line increases with a slope that flattens out as the number of threads increases, but the increase is still visibly present. However, lab2_list-4.png has a blue line that is mainly flat, even decreasing ever so slightly. This can be attributed to the fact that my code for the add operations locks and unlocks the mutex around each iteration of an add +1 and add -1, as seen in the add_mutex function. My code for the list operations locks and unlocks the mutex around the entire per-thread code segment that inserts elements, gets list length, looks up and deletes keys. Since more operations are done per lock/unlock for Part-2, less overhead is incurred by switching contexts between threads.

2.2.2 - scalability of spin locks
    For add operations, the time per protected operation increases more with spin-lock protection than with mutex protection as the number of threads increases. They start out with the same cost for 1 thread before spin-locks show an acceleration in cost. For list operations, cost of spin-lock protected operations actually starts off lower than mutex protected for 1 thread and quickly increases to cost more than mutex at around 4 threads. Spin lock inefficiency is because they take up CPU cycles spinning when waiting for control of the lock, instead of being blocked to save CPU cycles like mutexes do. As thread number increases, more threads are left spinning, showing accelerating costs. The general increase in cost with thread increase can be attributed to overhead of switching contexts between threads as mentioned in 2.2.1.

SOURCES
- pthreads: https://computing.llnl.gov/tutorials/pthreads/
- dynamically allocate pthread array: https://stackoverflow.com/questions/26753957/how-to-dynamically-allocateinitialize-a-pthread-array
- clock_gettime(2): https://man7.org/linux/man-pages/man2/clock_gettime.2.html
- atomic builtins: https://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html
- gnuplot(1): https://linux.die.net/man/1/gnuplot
- sched_yield(2): https://man7.org/linux/man-pages/man2/sched_yield.2.html
- spin lock implementation: https://attractivechaos.wordpress.com/2011/10/06/multi-threaded-programming-efficiency-of-locking/
- __sync_val_compare_and_swap example: https://stackoverflow.com/questions/25647710/atomic-get-load-and-set-store-with-older-gcc-sync-builtins
- strcmp: https://www.tutorialspoint.com/c_standard_library/c_function_strcmp.htm
